---
layout: post
title: "Study - Course 4: Convolutional Neural Networks"
date: 2018-11-12
---
source: [deeplearning.ai](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)

## Padding
* Valid convolution: no padding
* Same convolution: Pad so that output size is the same as the input size

## Summary of convolutions

* **n x n** image __f x f__ filter
* padding __p__
* stride __s__
* (n + 2p - f )/s + 1

## MaxPooling and AveragePooling
* Hyperparameters:
1. filter, normally : f=2
2. stride, normally: s=2
* No parameters to learn

## Why convolutions
* Parameter sharing
* Sparsity of connections

## Classic networks

### LeNet-5  1998
Conv -- Pool -- Conv -- Pool -- FC
### AlexNet  2012
Pros:
1. ~60 million parameters
2. ReLu activation
Not used today:
1. Multiple GPUs
2. Local Response Normalization

### VGG-16 2015
conv 3x3 filter s =1 same padding
max pool 2x2 s =2
simplify the architecture
1. ~138 million parameters

## ResNet 152 layers
__skip connections__
* Enable the network to get deeper without harming the performance of the network. When the network gets deeper and deeper, it's difficult to choose parameters to learn idendity function, while using the skip connections will solve this proble.
* Use the skip connections allows to add extra layers without hurting the performance of the network
* Skip connection also allows the gradient to be directly backpropagated to earlier layers.
__Residuel network__


## Inception 2014
> * How to solve the problem of computational cost: add a 1x1 conv layer (bottleneck) to reduce the computational cost.
* The inception module do the convolution and max pooling in the same time and concatenate the result.
* Additional side branches: take some hidden layer to make some prediction. prevent overfitting

* You can use 1x1 Conv layer to reduce nc, but not nh, nw
* You can use pooling layer to reduce nh, nw, but not nc

## Transfer learning
Download weights that someone else has already trained on the network architecture and use that as pre-training. And transfer that to a new task that I might be interested in.
Use it as an initial neural network.
1. Download some open source implementation of a neural network, download not just the code but also the __weights__.
* If you have a very small training set, you can freeze all the parameters in the earlier layers and train only your own layer.
* If you have a larger training set, you can freeze some layers and train the other layers. Or add your own hidden layers.
* If you have a lot of data, you can take the open source network and weights, and use the whole thing just as initialization and train the whole network.

## Data Augmentation
- __*Mirroring*__
- __*Random Cropping*__
- Rotation 
- Shearing
- Local warping

- __*Color shifting*__
> PCA(AlexNet paper) color Augmentation

## Two sources of knowledge
* Labeled Data
* Hand engineered features/network architecture/other components
For computer vision, there is not enough data, so it relys more on hand engineering.

## Doing well on benchmarks/winning competitions
* Ensembling (time consuming) -- Train several networks independently and average their outputs. yhat.
* Multi-crop at test time (time consuming) -- Run classifier on multiple versions of test images and average results

DO NOT USE IT FOR PRODUCT

## Use open source code
* use architectures of networks published in the literature
* use open source implementation if possible
* use pretrained models and fine-tune on your dataset
