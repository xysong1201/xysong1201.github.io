---
layout: post
title: "Week #21"
date: 2019-03-18
categories: [Summary of weeks]
---
### Results

|Gourp Id|Parameter num|initial lr|step_size|gamma|epoch|Loss|Figure|
|0|13177026|0.001|100|0.8|350|0.0402|<img src="{{ site.baseurl }}/assets/s0_1.jpg" height="350" width="450">|
|1|13177026|0.001|100|0.85|750|0.1314|<img src="{{ site.baseurl }}/assets/s1_3.jpg" height="350" width="450">|
|2|13177026|0.001|100|0.85|950|0.52985|<img src="{{ site.baseurl }}/assets/s2_2.jpg" height="350" width="450">|
|3|13177026|0.001|100|0.85|900|1.23616|<img src="{{ site.baseurl }}/assets/s3_1.jpg" height="350" width="450">|
|3|52633794|0.001|100|0.85|450|0.979685|<img src="{{ site.baseurl }}/assets/s3_2.jpg" height="350" width="450">|

### Try a simple network
After discussing with Prof. Xavier, with his recommendation, I tried to conduct the experiments using the simpliest *Conv-BatchNorm-Relu* concatenated architecture. The experiments are as belows:

1. Group 0, simple network with 7 layers, with 7,932,630 parameters.
2. Group 1, simple network with 4 layers, with 7,932,630 parameters.
3. Group 2, simple network with 4 layers, with 298,710 parameters.
4. Group 2, simple network with 7 layers, with 7,932,630 parameters.
5. Group 3, simple network with 7 layers, with 7,932,630 parameters.

Since we have the experiments done with QuickNAT using the group 0,1,2,3. So here the same group number is used for comparison.

### Paper Construction
We concluded that the paper should be constructed in order to find out the contributions we want to make.
