---
layout: post
title: "cs231n study note [In Mandarin]"
categories: [Study notes]
---
1. Score function 是用来把image中的pixel value转化为class score的。
2. Loss是用来衡量training data里面预测的class score与ground truth label的接近程度的，也可以说，loss是衡量一组weight是否很好的一个标准。
3. Optimization指的是找到那个能使得loss很小的那组weight值的过程（process)。所以说optimization其实还是一个process，是一个过程，目的就是找能让loss最小的那个weight。

**目标**：得到一组weight值，使得x_i的预测的score与groundtruth一致，也就是说loss值很小。

方法一：随机找。也就是说用一个loop，标记出能够让loss最小的那个weight值，不断更新这个值。  
想法：从一个随机的weight值开始，每次不停地迭代，每次都找到一个更好的weight，然后不断循环这个过程。  
方法二：随机找个方向，然后迈一小步，如果得到的loss值减小了，那就证明这个新的weight是更好的，把loss值记录下来，然后再重复刚才的过程。  
方法三：在方法二中，我们随机选择方向来减小loss值，其实可以直接通过计算 **loss function的斜率** 来找到能使loss减小的最快的方向。也就是说，**loss function的斜率的负数越大，loss下降的越快**。

有两种计算斜率的方法，一种叫做numerical gradient，另一种叫做analytic gradient。
* numerical gradient是一种近似值，但是容易计算。
* analytic gradient更准确，但是容易出错，而且需要大的计算量。

pytorch里面 `autograd` package提供了automatic differentiation for all operations on Tensors.  
backprop is defined by how your code is run, and every single iteration can be different.  
如果tensor的`requires_grad`被设置为`True`，那么就会track tensor的所有的operation。当完成计算直接调用 `.backward()`，则所有的gradient都会自动计算。
这个tensor的gradient会被累积到`.grad` attribute里面。    
如果不想让tensor记录计算的历史，可以使用`.detach()`  
也可以用`with torch.no_grad():`  
`tensor`和`Function`是连接的，组成了一个循环图，编码了所有的计算的历史。每一个tensor都有一个`.grad_fn`属性，这个`grad_fn`连接了创造这个`Tensor`的`Function`（不包括由用户创建的tensor，用户创建的tensor的`grad_fn` is `None`）
