---
layout: post
title: "Hyperparameter tuning, Regularization and Optimization"
categories: [Study notes]
---

### Initialization
* Different initializations lead to different results
* Random initialization is used to break symmetry and make sure different hidden units can learn different things
* Don't intialize to values that are too large
* He initialization works well for networks with ReLU activations.
